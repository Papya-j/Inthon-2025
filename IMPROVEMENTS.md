# ğŸš€ Baseline ëª¨ë¸ ê°œì„  ì „ëµ ë° êµ¬í˜„

## ğŸ“Š ê°œì„  ìš”ì•½

### **Phase 1: ë°ì´í„° ì „ëµ (ê°€ì¥ ì¤‘ìš”!)**
| í•­ëª© | Baseline | ê°œì„  | ì´ìœ  |
|------|----------|------|------|
| í•™ìŠµ ìƒ˜í”Œ ìˆ˜ | 500,000 | **1,000,000** | ë” ë§ì€ íŒ¨í„´ í•™ìŠµ |
| í•™ìŠµ ìë¦¿ìˆ˜ | 1~3ìë¦¬ | **1~5ìë¦¬** | OOD ê°„ê·¹ ìµœì†Œí™” (ê·œì • ìµœëŒ€ì¹˜) |
| ë³µì¡ë„ (depth) | 3 | **4** | ë³µì¡í•œ ìˆ˜ì‹ í•™ìŠµ |
| ê²€ì¦ ìƒ˜í”Œ ìˆ˜ | 128 | **256** | ë” ì‹ ë¢°ì„± ìˆëŠ” í‰ê°€ |

**í•µì‹¬**: í•™ìŠµ ë°ì´í„°ë¥¼ 1~3ìë¦¬ â†’ 1~5ìë¦¬ë¡œ í™•ëŒ€í•˜ë©´, í‰ê°€ ì‹œ 6ìë¦¬+ OOD ì¼ë°˜í™” ì„±ëŠ¥ì´ **ëŒ€í­ í–¥ìƒ**ë©ë‹ˆë‹¤!

---

### **Phase 2: ì•„í‚¤í…ì²˜ ê°œì„ **

#### **Transformer vs GRU**
```
GRU (Baseline):
  ì…ë ¥ ì „ì²´ â†’ ë‹¨ì¼ ë²¡í„° â†’ ì¶œë ¥
  âŒ ë³‘ëª©: ê¸´ ì‹œí€€ìŠ¤ ì •ë³´ ì†ì‹¤
  âŒ ìœ„ì¹˜ ì •ë³´ ë¶€ì¡±

Transformer (ê°œì„ ):
  ì…ë ¥ ì „ì²´ â† Self-Attention â†’ ì¶œë ¥
  âœ… ëª¨ë“  ìœ„ì¹˜ ì°¸ì¡° ê°€ëŠ¥
  âœ… Positional Encoding (ìœ„ì¹˜ ì •ë³´ ëª…ì‹œì )
  âœ… ë³‘ë ¬ í•™ìŠµ ê°€ëŠ¥
```

| í•­ëª© | Baseline (GRU) | ê°œì„  (Transformer) |
|------|----------------|-------------------|
| ì•„í‚¤í…ì²˜ | GRU Encoder-Decoder | Transformer Encoder-Decoder |
| d_model | 256 | **512** |
| Layers | 1 | **4 encoder + 4 decoder** |
| Attention | âŒ | âœ… Multi-head (8 heads) |
| Positional Encoding | âŒ | âœ… Learnable |
| íŒŒë¼ë¯¸í„° ìˆ˜ | ~800K | **~25M** |

---

### **Phase 3: í•™ìŠµ ìµœì í™”**

| í•­ëª© | Baseline | ê°œì„  | íš¨ê³¼ |
|------|----------|------|------|
| Learning Rate | 2e-3 | **3e-4** | ì•ˆì •ì  í•™ìŠµ |
| LR Schedule | ê³ ì • | **Warmup + Cosine** | ë¹ ë¥¸ ìˆ˜ë ´ |
| Label Smoothing | 0.0 | **0.1** | Overconfidence ë°©ì§€ |
| Batch Size | 128 | **64** | Transformer ë©”ëª¨ë¦¬ ìµœì í™” |
| Epochs | 4~50 | **10** | TransformerëŠ” ë¹ ë¥´ê²Œ ìˆ˜ë ´ |
| Max Output Length | 32 | **50** | ê¸´ ìˆ«ì ì¶œë ¥ ê°€ëŠ¥ |

---

## ğŸ¯ ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 

### **í˜„ì¬ Baseline ì„±ëŠ¥**
- EM (Exact Match): **36.7%**
- TES (Token Edit Similarity): **59.2%**

### **ê°œì„  ëª©í‘œ**
```
ë‹¨ê³„ë³„ ì˜ˆìƒ ì„±ëŠ¥:

1. ë°ì´í„°ë§Œ ê°œì„  (1~5ìë¦¬):
   EM: 36.7% â†’ 55~60% (+18~23%)
   
2. Transformer ì¶”ê°€:
   EM: 55~60% â†’ 70~75% (+15%)
   
3. í•™ìŠµ ìµœì í™” ì¶”ê°€:
   EM: 70~75% â†’ 80~85% (+10%)

ìµœì¢… ëª©í‘œ: EM 80%+ (í˜„ì¬ ëŒ€ë¹„ 2.2ë°° í–¥ìƒ)
```

---

## ğŸ“ ì£¼ìš” ë³€ê²½ì‚¬í•­ ìƒì„¸

### **1. config.py**

```python
# ModelConfig í™•ì¥
@dataclass
class ModelConfig:
    d_model: int = 512              # 256 â†’ 512
    nhead: int = 8                  # NEW
    num_encoder_layers: int = 4     # NEW
    num_decoder_layers: int = 4     # NEW
    dim_feedforward: int = 2048     # NEW
    dropout: float = 0.1            # NEW
    use_transformer: bool = True    # NEW

# TrainConfig ê°œì„ 
@dataclass
class TrainConfig:
    lr: float = 3e-4               # 2e-3 â†’ 3e-4
    warmup_steps: int = 1000       # NEW
    valid_every: int = 200         # 50 â†’ 200
    max_gen_len: int = 50          # 32 â†’ 50
    num_epochs: int = 10           # 4 â†’ 10
    label_smoothing: float = 0.1   # NEW
    grad_clip: float = 1.0         # NEW
```

### **2. model.py - TransformerSeq2Seq ì¶”ê°€**

```python
class TransformerSeq2Seq(nn.Module):
    """Transformer ê¸°ë°˜ Seq2Seq (ì„±ëŠ¥ ê°œì„  ë²„ì „)"""
    
    def __init__(self, in_vocab, out_vocab, **kwargs):
        # ì„ë² ë”©
        self.embed_in = nn.Embedding(in_vocab, d_model)
        self.embed_out = nn.Embedding(out_vocab, d_model)
        
        # Positional Encoding (í•™ìŠµ ê°€ëŠ¥)
        self.pos_encoder = nn.Embedding(512, d_model)
        self.pos_decoder = nn.Embedding(512, d_model)
        
        # Transformer
        self.encoder = nn.TransformerEncoder(...)
        self.decoder = nn.TransformerDecoder(...)
        
        # Output
        self.out_proj = nn.Linear(d_model, out_vocab)
```

**í•µì‹¬ ê°œì„ ì **:
- âœ… Multi-head Self-Attention (ì „ì²´ ì‹œí€€ìŠ¤ ì°¸ì¡°)
- âœ… Positional Encoding (ìœ„ì¹˜ ì •ë³´ ëª…ì‹œì  í•™ìŠµ)
- âœ… Feed-Forward Networks (ë¹„ì„ í˜• ë³€í™˜ ê°•í™”)
- âœ… Layer Normalization (ì•ˆì •ì  í•™ìŠµ)
- âœ… Residual Connections (Gradient flow ê°œì„ )

### **3. train.py - í•™ìŠµ ë£¨í”„ ê°œì„ **

```python
# ë°ì´í„° ê°œì„ 
train_dataset = ArithmeticDataset(
    num_samples=1_000_000,  # 500K â†’ 1M
    max_depth=4,            # 3 â†’ 4
    num_digits=(1, 5),      # (1,3) â†’ (1,5) â­ í•µì‹¬!
)

# Learning Rate Scheduler
def get_lr(step, warmup_steps, max_steps, base_lr):
    if step < warmup_steps:
        return base_lr * (step + 1) / warmup_steps  # Linear warmup
    else:
        progress = (step - warmup_steps) / (max_steps - warmup_steps)
        return base_lr * 0.5 * (1 + cos(progress * Ï€))  # Cosine annealing

# Label Smoothing
loss_fn = nn.CrossEntropyLoss(
    ignore_index=pad_id,
    label_smoothing=0.1,  # NEW
)
```

---

## ğŸ”¬ ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­

### **Transformerì˜ Attention ë©”ì»¤ë‹ˆì¦˜**

```
ì…ë ¥: "12+34"

Step 1: Self-Attention (Encoder)
  '1' â† ëª¨ë“  í† í° ì°¸ì¡° â†’ ['1', '2', '+', '3', '4']
  '2' â† ëª¨ë“  í† í° ì°¸ì¡° â†’ ['1', '2', '+', '3', '4']
  '+' â† ëª¨ë“  í† í° ì°¸ì¡° â†’ ['1', '2', '+', '3', '4']
  ...
  
ê²°ê³¼: ê° ìœ„ì¹˜ê°€ í•„ìš”í•œ ì •ë³´ë¥¼ ì„ íƒì ìœ¼ë¡œ ê°€ì ¸ì˜´

Step 2: Cross-Attention (Decoder)
  ì¶œë ¥ '4' â† ì…ë ¥ ì „ì²´ ì°¸ì¡° â†’ ['1', '2', '+', '3', '4']
  ì¶œë ¥ '6' â† ì…ë ¥ ì „ì²´ ì°¸ì¡° â†’ ['1', '2', '+', '3', '4']
  
ê²°ê³¼: ì¶œë ¥ì´ ì…ë ¥ì˜ ì–´ë–¤ ë¶€ë¶„ê³¼ ê´€ë ¨ë˜ëŠ”ì§€ í•™ìŠµ
```

### **Positional Encodingì˜ ì—­í• **

```
ì…ë ¥: "123" vs "321"

Positional Encoding ì—†ìœ¼ë©´:
  embed('1') + embed('2') + embed('3') = ê°™ì€ í‘œí˜„ (ìˆœì„œ ë¬´ì‹œ)
  
Positional Encoding ìˆìœ¼ë©´:
  embed('1') + pos(0) + embed('2') + pos(1) + embed('3') + pos(2)
  â†’ ìœ„ì¹˜ ì •ë³´ ëª…ì‹œì ìœ¼ë¡œ í•™ìŠµ
  â†’ "ì²« ë²ˆì§¸ ìë¦¿ìˆ˜", "ë°±ì˜ ìë¦¬" ë“± ê°œë… í•™ìŠµ ê°€ëŠ¥
```

---

## ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒì˜ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜

### **1. OOD ê°„ê·¹ ê°ì†Œ**
```
Baseline:
  í•™ìŠµ: 1~3ìë¦¬ (ì˜ˆ: 1, 12, 123)
  í‰ê°€: 6ìë¦¬+ (ì˜ˆ: 123456)
  ê°„ê·¹: 3ìë¦¬ â†’ 6ìë¦¬ (2ë°° ë„ì•½) âŒ

ê°œì„ :
  í•™ìŠµ: 1~5ìë¦¬ (ì˜ˆ: 1, 12, 123, 1234, 12345)
  í‰ê°€: 6ìë¦¬+ (ì˜ˆ: 123456)
  ê°„ê·¹: 5ìë¦¬ â†’ 6ìë¦¬ (1.2ë°° ë„ì•½) âœ…
  
ê²°ê³¼: ëª¨ë¸ì´ 5ìë¦¬ íŒ¨í„´ì„ í•™ìŠµ â†’ 6ìë¦¬ë¡œ ì¼ë°˜í™” í›¨ì”¬ ì‰¬ì›€!
```

### **2. Attentionì˜ íš¨ê³¼**
```
ë¬¸ì œ: "2+3*4" = 14 (ì—°ì‚°ì ìš°ì„ ìˆœìœ„)

GRU:
  '2' â†’ h1 â†’ h2 â†’ h3 â†’ h4 â†’ h5 (ìˆœì°¨ì  ì••ì¶•)
  h5 í•˜ë‚˜ë¡œ ëª¨ë“  ì •ë³´ í‘œí˜„ âŒ
  
Transformer:
  ì¶œë ¥ ìƒì„± ì‹œ:
  - Attention('*') â†’ '+' ë³´ë‹¤ ìš°ì„ 
  - '3'ê³¼ '4'ë¥¼ ë¨¼ì € ê²°í•©
  - ê²°ê³¼ë¥¼ '2'ì™€ í•©ì‚°
  âœ… ì—°ì‚°ì ìš°ì„ ìˆœìœ„ ëª…ì‹œì  í•™ìŠµ!
```

### **3. Positional Encoding íš¨ê³¼**
```
ë¬¸ì œ: ìë¦¿ìˆ˜ ì˜¬ë¦¼/ë‚´ë¦¼

"999+1" = 1000

Positional Encoding ìˆìœ¼ë©´:
  pos(0): ì¼ì˜ ìë¦¬ â†’ ì˜¬ë¦¼ ë°œìƒ í•™ìŠµ
  pos(1): ì‹­ì˜ ìë¦¬ â†’ ì˜¬ë¦¼ ì „íŒŒ í•™ìŠµ
  pos(2): ë°±ì˜ ìë¦¬ â†’ ì˜¬ë¦¼ ì „íŒŒ í•™ìŠµ
  pos(3): ì²œì˜ ìë¦¬ â†’ ìƒˆ ìë¦¿ìˆ˜ ìƒì„± í•™ìŠµ
  
âœ… ìë¦¿ìˆ˜ë³„ ë…ë¦½ì  ì²˜ë¦¬ ê°€ëŠ¥!
```

---

## ğŸ¯ ëŒ€íšŒ í‰ê°€ ì§€í‘œë³„ ì „ëµ

### **1. Calculation Accuracy (35%)**
- âœ… 1~5ìë¦¬ í•™ìŠµìœ¼ë¡œ ê¸°ë³¸ ì—°ì‚° ì •í™•ë„ í–¥ìƒ
- âœ… Transformerë¡œ ë³µì¡í•œ ìˆ˜ì‹ ì²˜ë¦¬

### **2. Law Preservation (20%)**
- âœ… Attentionìœ¼ë¡œ ì—°ì‚°ì ìš°ì„ ìˆœìœ„ í•™ìŠµ
- âœ… ë” ë§ì€ ìƒ˜í”Œë¡œ êµí™˜/ê²°í•© ë²•ì¹™ í•™ìŠµ

### **3. Expression Consistency (30%)**
- âœ… Positional Encodingìœ¼ë¡œ êµ¬ì¡°ì  ì´í•´
- âœ… Self-Attentionìœ¼ë¡œ ë™ì¹˜ í‘œí˜„ ì¸ì‹

### **4. Relational Consistency (15%)**
- âœ… ìë¦¿ìˆ˜ ì •ë³´ ëª…ì‹œì  í•™ìŠµ
- âœ… ì¶œë ¥ ê°„ ê´€ê³„ ìœ ì§€ ëŠ¥ë ¥ í–¥ìƒ

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

```bash
# 1. ê°œì„ ëœ ëª¨ë¸ í•™ìŠµ
python3 train.py

# ì˜ˆìƒ í•™ìŠµ ì‹œê°„:
# - CPU: ~8ì‹œê°„
# - GPU (CUDA): ~1ì‹œê°„

# 2. ë¡œì»¬ í…ŒìŠ¤íŠ¸
python3 local_test.py .

# 3. ì²´í¬í¬ì¸íŠ¸ í™•ì¸
ls -lh best_model.pt
```

---

## ğŸ“Š ì˜ˆìƒ ê²°ê³¼

```
Epoch 1: EM ~50% (ê¸°ë³¸ íŒ¨í„´ í•™ìŠµ)
Epoch 3: EM ~65% (ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ)
Epoch 5: EM ~75% (ì¼ë°˜í™” ì‹œì‘)
Epoch 8: EM ~80%+ (ëª©í‘œ ë‹¬ì„±)

ìµœì¢… ì²´í¬í¬ì¸íŠ¸:
- íŒŒì¼ í¬ê¸°: ~100MB (Baseline 3MB â†’ 33ë°°)
- EM: 80~85%
- TES: 90~95%
```

---

## ğŸ’¡ ì¶”ê°€ ê°œì„  ì•„ì´ë””ì–´ (ì‹œê°„ ìˆì„ ë•Œ)

1. **Beam Search**: Greedy â†’ Beam Search (k=5)
2. **Data Augmentation**: ê´„í˜¸ ì¶”ê°€/ì œê±°, ì—°ì‚°ì êµí™˜
3. **Curriculum Learning**: ì‰¬ìš´ ë¬¸ì œ â†’ ì–´ë ¤ìš´ ë¬¸ì œ
4. **Auxiliary Loss**: ì¤‘ê°„ ê³„ì‚° ë‹¨ê³„ ì˜ˆì¸¡
5. **Larger Model**: d_model 512 â†’ 768
6. **More Data**: 1M â†’ 2M ìƒ˜í”Œ

---

## ğŸ‰ ê²°ë¡ 

ì´ë²ˆ ê°œì„ ìœ¼ë¡œ:
- âœ… **ë°ì´í„°**: 1~5ìë¦¬ê¹Œì§€ í•™ìŠµ (OOD ê°„ê·¹ ìµœì†Œí™”)
- âœ… **ì•„í‚¤í…ì²˜**: Transformer (ì •ë³´ ë³‘ëª© í•´ê²°)
- âœ… **í•™ìŠµ**: Warmup + Label Smoothing (ì•ˆì •ì  ìµœì í™”)

**ì˜ˆìƒ ì„±ëŠ¥**: EM 36.7% â†’ **80%+** (2.2ë°° í–¥ìƒ)

**ëŒ€íšŒ ëª©í‘œ**: ìˆ˜í•™ì  ì¼ë°˜í™” ëŠ¥ë ¥ ê²€ì¦ â†’ **ë‹¬ì„± ê°€ëŠ¥!**
