from __future__ import annotations
from typing import List, Any, Tuple
from config import TrainConfig, ModelConfig, TokenizerConfig

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, IterableDataset
from tqdm import tqdm

from dataloader import (
    ArithmeticDataset,  # ì‚¬ì¹™ì—°ì‚° ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ì£¼ëŠ” Dataset
    get_dataloader,     # Datasetì„ ë°›ì•„ì„œ DataLoaderë¡œ ë°”ê¿”ì£¼ëŠ” í•¨ìˆ˜
)
from do_not_edit.metric import compute_metrics  # EM, TES ê°™ì€ ê°„ë‹¨í•œ ì„±ëŠ¥ ì§€í‘œ

from model import (
    TinySeq2Seq,
    CharTokenizer,      # ë¬¸ì ë‹¨ìœ„ í† í¬ë‚˜ì´ì €
    tokenize_batch,     # batch(dict)ë¥¼ í† í¬ë‚˜ì´ì¦ˆ + íŒ¨ë”©ê¹Œì§€ í•´ì£¼ëŠ” í•¨ìˆ˜
    INPUT_CHARS,        # ì…ë ¥ ë¬¸ì ì§‘í•©
    OUTPUT_CHARS,       # ì¶œë ¥ ë¬¸ì ì§‘í•©
)


# ======================================================================================
# 1. í•™ìŠµ ë£¨í”„
# ======================================================================================
def train_loop(
    model: nn.Module,           # í•™ìŠµí•  ëª¨ë¸
    dataloader: DataLoader,     # DataLoaderë¥¼ ì§ì ‘ ì „ë‹¬ë°›ì•„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    input_tokenizer: CharTokenizer,      # (tokenize_batch ìœ í‹¸ë¦¬í‹°ë¥¼ ìœ„í•´ ìœ ì§€)
    output_tokenizer: CharTokenizer,     # ì¶œë ¥ ë¬¸ì í† í¬ë‚˜ì´ì €
    device: torch.device,       # cpu ë˜ëŠ” cuda
    val_dataloader: DataLoader | None = None,  # ë³„ë„ ê²€ì¦ DataLoader (Noneì´ë©´ í›ˆë ¨ ë°°ì¹˜ë¡œ ê²€ì¦)
    *,
    train_config: TrainConfig,  # í•™ìŠµ ì„¤ì •
    model_config: ModelConfig,  # ëª¨ë¸ ì„¤ì •
    tokenizer_config: TokenizerConfig,  # í† í¬ë‚˜ì´ì € ì„¤ì •
):
    # ëª¨ë¸ì„ GPU/CPUë¡œ ë³´ëƒ„
    model.to(device)

    # ì˜µí‹°ë§ˆì´ì €: AdamWëŠ” Adam + weight decayê°€ ë“¤ì–´ê°„ ë²„ì „
    optim = torch.optim.AdamW(model.parameters(), lr=train_config.lr)
    
    # ğŸ”¥ Learning Rate Scheduler (Warmup + Cosine Annealing)
    def get_lr(step, warmup_steps, max_steps, base_lr):
        if step < warmup_steps:
            # Linear warmup
            return base_lr * (step + 1) / warmup_steps
        else:
            # Cosine annealing
            progress = (step - warmup_steps) / (max_steps - warmup_steps)
            return base_lr * 0.5 * (1 + torch.cos(torch.tensor(progress * 3.14159)))
    
    # ì´ ìŠ¤í… ìˆ˜ ê³„ì‚°
    total_steps = len(dataloader) * train_config.num_epochs
    
    # ğŸ”¥ Label Smoothing ì ìš©
    # pad í† í°ì€ ë¬´ì‹œí•˜ë„ë¡(ignore_index) ì„¤ì •
    loss_fn = nn.CrossEntropyLoss(
        ignore_index=output_tokenizer.pad_id,
        label_smoothing=train_config.label_smoothing,  # Label smoothing ì¶”ê°€
    )

    step = 0
    model.train()  # í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜ (Dropout ë“± ì¼œì§)

    # tqdmì€ ì§„í–‰ ìƒí™©ì„ ì˜ˆì˜ê²Œ ë³´ì—¬ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.
    pbar = tqdm(total=train_config.max_train_steps if train_config.max_train_steps is not None else None, desc="train", unit="step", ncols=120, dynamic_ncols=True, leave=True)

    # best EM ì¶”ì ìš© ë³€ìˆ˜ (Noneì´ ì•„ë‹ˆë©´ ê°œì„  ì‹œ ëª¨ë¸ ì €ì¥)
    best_em = float("-inf")

    for epoch in range(train_config.num_epochs):
        # max_train_steps ì œí•œì´ ìˆì„ ì‹œ, ì œí•œì„ ë‹¤ ì±„ìš°ë©´ í•™ìŠµì„ ì¢…ë£Œí•©ë‹ˆë‹¤.
        if train_config.max_train_steps is not None and step >= train_config.max_train_steps: break
        pbar.write(f"Starting epoch {epoch + 1}/{train_config.num_epochs}")

        # ì‹¤ì œë¡œ ë°°ì¹˜ë¥¼ í•˜ë‚˜ì”© ë½‘ì•„ì„œ í•™ìŠµí•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.
        for batch in dataloader:
            # --------------------------------------------------------------
            # 1) í† í¬ë‚˜ì´ì¦ˆ & í…ì„œë¡œ ë³€í™˜
            #    `tokenize_batch`ëŠ” BatchTensors(src, tgt_inp, tgt_out)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
            #    ë³€ìˆ˜ ì—­í• :
            #      - `src` (encoder input): ëª¨ë¸ì˜ ì¸ì½”ë” ì…ë ¥. ì •ìˆ˜ í…ì„œ, shape (B, S).
            #      - `target_input` (tgt_inp): ë””ì½”ë”ì— teacher-forcingìœ¼ë¡œ ë„£ëŠ” ì…ë ¥. shape (B, T).
            #          ì¼ë°˜ì ìœ¼ë¡œ BOSë¥¼ ì•ì— ë¶™ì´ê³  EOSëŠ” ì œì™¸í•œ ì‹œí€€ìŠ¤ì…ë‹ˆë‹¤.
            #      - `target_output` (tgt_out): ë””ì½”ë”ê°€ ì˜ˆì¸¡í•´ì•¼ í•˜ëŠ” ì •ë‹µ(ì†ì‹¤ ëŒ€ìƒ). shape (B, T).
            #          ì¼ë°˜ì ìœ¼ë¡œ target_inputì—ì„œ BOSë¥¼ ëº€ ê²ƒì— EOSë¥¼ ë¶™ì¸ í˜•íƒœì…ë‹ˆë‹¤.
            #    ì˜ˆì‹œ (í† í° idê°€ ë‹¤ìŒê³¼ ê°™ë‹¤ê³  ê°€ì •):
            #      bos_id=1, eos_id=2, '1'->5, '6'->6
            #      ì›ë³¸ target_text: "16"
            #      target_input ids:  [1, 5, 6]    # [BOS, '1', '6']
            #      target_output ids: [5, 6, 2]    # ['1', '6', EOS']
            #    ì£¼ì˜: ëª¨ë“  í…ì„œëŠ” dtype=torch.longì´ê³  `.to(device)`ë¡œ ëª…ì‹œì  ì´ë™ì´ í•„ìš”í•©ë‹ˆë‹¤.
            # --------------------------------------------------------------
            batch_tensors = tokenize_batch(batch, input_tokenizer, output_tokenizer)
            src = batch_tensors.src.to(device)
            target_input = batch_tensors.tgt_inp.to(device)
            target_output = batch_tensors.tgt_out.to(device)

            # Forward: ëª¨ë¸ì— ì…ë ¥ì„ ì „ë‹¬í•˜ê³  ì¶œë ¥ì„ ì–»ìŠµë‹ˆë‹¤.
            # ì¶œë ¥ì€ (B, T, V) í˜•íƒœë¡œ ë°˜í™˜ë©ë‹ˆë‹¤.
            logits = model(src, target_input, input_tokenizer.pad_id)

            # --------------------------------------------------------------
            # 4) Loss ê³„ì‚°
            # --------------------------------------------------------------
            loss = loss_fn(
                logits.view(-1, logits.size(-1)),  # (B*T, V)
                target_output.view(-1),             # (B*T,)
            )

            # --------------------------------------------------------------
            # 5) Backward + optimizer step
            # --------------------------------------------------------------
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), train_config.grad_clip)
            optim.step()
            optim.zero_grad()
            
            # ğŸ”¥ Learning Rate ì—…ë°ì´íŠ¸ (Warmup + Cosine Annealing)
            new_lr = get_lr(step, train_config.warmup_steps, total_steps, train_config.lr)
            for param_group in optim.param_groups:
                param_group['lr'] = new_lr

            step += 1 

            # --------------------------------------------------------------
            # 6) Validation
            # --------------------------------------------------------------
            if step % train_config.valid_every == 0:
                model.eval()  # í‰ê°€ ëª¨ë“œ
                # ê²€ì¦ ë°ì´í„°ì…‹ì„ ìˆœíšŒí•˜ë©° ê° ë°°ì¹˜ì— ëŒ€í•´ ê²€ì¦ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                with torch.no_grad():
                    preds_all: List[str] = []
                    targets_all: List[str] = []
                    inputs_all: List[str] = [] # ê²€ì¦ ë°ì´í„°ì…‹ì˜ ì…ë ¥, ì •ë‹µ, ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

                    for val_batch in val_dataloader: # ê²€ì¦ ë°ì´í„°ì…‹ì„ ìˆœíšŒí•˜ë©° ê° ë°°ì¹˜ì— ëŒ€í•´ ê²€ì¦ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                        val_bt = tokenize_batch(val_batch, input_tokenizer, output_tokenizer)
                        val_src = val_bt.src.to(device)
                        gen_ids = model.generate(
                            src=val_src,
                            max_len=train_config.max_gen_len,
                            bos_id=output_tokenizer.bos_id,
                            eos_id=output_tokenizer.eos_id,
                            src_pad_id=input_tokenizer.pad_id,
                        )

                        for i in range(gen_ids.size(0)):
                            seq_chars: List[str] = []
                            for t in gen_ids[i].tolist():
                                idx = int(t)
                                if idx == output_tokenizer.eos_id:
                                    break
                                if idx in output_tokenizer.itos:
                                    ch = output_tokenizer.itos[idx]
                                    if ch.isdigit() or (ch == '-' and not seq_chars):
                                        seq_chars.append(ch)
                            pred_str = "".join(seq_chars)
                            if pred_str == "-":
                                pred_str = ""
                            preds_all.append(pred_str)
                        # ê²€ì¦ ë°ì´í„°ì…‹ì˜ ì •ë‹µ, ì…ë ¥ì„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.
                        targets_all.extend(val_batch["target_text"])
                        inputs_all.extend(val_batch["input_text"])

                    # ê²€ì¦ ë°ì´í„°ì…‹ì˜ ì˜ˆì¸¡, ì •ë‹µì„ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
                    em_batch = compute_metrics(preds_all, targets_all)

                    # ì§„í–‰ë°”ì—ë„ ì„±ëŠ¥ì„ í‘œì‹œí•©ë‹ˆë‹¤.
                    pbar.write(f"[valid {step}] EM={em_batch['EM']:.3f} TES={em_batch['TES']:.3f}")
                    pbar.set_postfix(
                        EM=f"{em_batch['EM']:.3f}",
                        TES=f"{em_batch['TES']:.3f}",
                    )
                    
                    pbar.refresh()

                    # ìµœê³  ì„±ëŠ¥ ê°±ì‹  ì‹œ ì „ì²´ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                    if train_config.save_best_path is not None:
                        current_em = float(em_batch.get("EM", -1.0))
                        if current_em > best_em:
                            best_em = current_em
                            # ì„¸ configë¥¼ dictë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
                            ckpt = {
                                "model_state": model.state_dict(),
                                "optim_state": optim.state_dict(),
                                "step": step,
                                "train_config": train_config.__dict__,  # í•™ìŠµ ì„¤ì • ì €ì¥
                                "model_config": model_config.__dict__,  # ëª¨ë¸ ì„¤ì • ì €ì¥
                                "tokenizer_config": tokenizer_config.__dict__,  # í† í¬ë‚˜ì´ì € ì„¤ì • ì €ì¥
                            }
                            torch.save(ckpt, train_config.save_best_path)
                            pbar.write(f"New best EM={best_em:.3f} at step {step}; saved to {train_config.save_best_path}")

                    B = len(preds_all) # ê²€ì¦ ë°ì´í„°ì…‹ì˜ í¬ê¸°
                    n_show = min(train_config.show_valid_samples, B)
                    pbar.write("Sample validation output:") # ì˜ˆì‹œë¡œ ëª‡ ê°œë§Œ ë³´ì—¬ì¤ë‹ˆë‹¤.
                    for i in range(n_show):
                        input_str = inputs_all[i]
                        tgt = targets_all[i]
                        pred = preds_all[i]
                        ok = "OK" if pred == tgt else "ERR"
                        pbar.write(f"  [{i}] {ok} | input: {input_str} | target: {tgt} | pred: {pred}")
                model.train()  # ë‹¤ì‹œ í•™ìŠµ ëª¨ë“œë¡œ

            # max_train_steps ì œí•œì´ ìˆì„ ì‹œ, ì œí•œì„ ë‹¤ ì±„ìš°ë©´ í•™ìŠµì„ ì¢…ë£Œí•©ë‹ˆë‹¤.
            if train_config.max_train_steps is not None and step >= train_config.max_train_steps:
                break # í•™ìŠµì„ ì¢…ë£Œí•©ë‹ˆë‹¤.

            pbar.update(1) # tqdm ì§„í–‰ 1 step

# ======================================================================================
# 2. main í•¨ìˆ˜
# ======================================================================================
def main():
    # GPUê°€ ìˆìœ¼ë©´ GPU, ì—†ìœ¼ë©´ CPU ì‚¬ìš©
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # --------------------------------------------------------------------------
    # 1) ë°ì´í„° ì¤€ë¹„ (ê°œì„ ëœ ì„¤ì •)
    # --------------------------------------------------------------------------
    
    # Train Dataset
    # ğŸ”¥ í•µì‹¬ ê°œì„ : 5ìë¦¬ê¹Œì§€ í•™ìŠµ (ê·œì • ìµœëŒ€ì¹˜), depth ì¦ê°€, ìƒ˜í”Œ ì¦ê°€
    train_dataset = ArithmeticDataset(
        num_samples=1_000_000,  # 500K â†’ 1M (ë” ë§ì€ í•™ìŠµ ë°ì´í„°)
        max_depth=4,            # 3 â†’ 4 (ë” ë³µì¡í•œ ìˆ˜ì‹)
        num_digits=(1, 5),      # (1,3) â†’ (1,5) (ê·œì • ìµœëŒ€ì¹˜, OOD ê°„ê·¹ ê°ì†Œ!)
        seed=123,
        mode="train",
    )

    # Train DataLoader
    train_dataloader = get_dataloader(
        train_dataset,
        batch_size=64,  # 128 â†’ 64 (TransformerëŠ” ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš©)
        num_workers=0,
        pin_memory=True,
    )

    # Validation Dataset
    # ê²€ì¦ì€ ë” ì–´ë µê²Œ (6ìë¦¬ê¹Œì§€ í¬í•¨)
    val_dataset = ArithmeticDataset(
        num_samples=256,        # 128 â†’ 256 (ë” ë§ì€ ê²€ì¦ ìƒ˜í”Œ)
        max_depth=5,            # 4 â†’ 5 (ë” ë³µì¡)
        num_digits=(1, 5),      # í•™ìŠµê³¼ ë™ì¼ (In-domain)
        seed=999,
        mode="val",
    )
    
    # Validation DataLoader, ìì„¸í•œ ì„¤ì •ì€ dataloader.pyë¥¼ ì°¸ê³ í•˜ì„¸ìš”.
    val_dataloader = get_dataloader(
        val_dataset,
        batch_size=128,
        num_workers=0,
        pin_memory=True,
    )

    # --------------------------------------------------------------------------
    # 2) í† í¬ë‚˜ì´ì € ì„¤ì • ì¤€ë¹„
    # --------------------------------------------------------------------------
    # í† í¬ë‚˜ì´ì € ì„¤ì • (ë³„ë„ë¡œ ê´€ë¦¬)
    tokenizer_config = TokenizerConfig(
        input_chars=INPUT_CHARS,
        output_chars=OUTPUT_CHARS,
        add_special=True,
    )

    # --------------------------------------------------------------------------
    # 3) í† í¬ë‚˜ì´ì € ì¤€ë¹„
    # --------------------------------------------------------------------------
    # ì…ë ¥ ë¬¸ì í† í¬ë‚˜ì´ì €, ì¶œë ¥ ë¬¸ì í† í¬ë‚˜ì´ì €ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤. ìì„¸í•œ ì„¤ì •ì€ model.pyë¥¼ ì°¸ê³ í•˜ì„¸ìš”.
    input_tokenizer = CharTokenizer(
        tokenizer_config.input_chars if tokenizer_config.input_chars is not None else INPUT_CHARS,
        add_special=tokenizer_config.add_special,
    )
    output_tokenizer = CharTokenizer(
        tokenizer_config.output_chars if tokenizer_config.output_chars is not None else OUTPUT_CHARS,
        add_special=tokenizer_config.add_special,
    )

    # --------------------------------------------------------------------------
    # 4) ëª¨ë¸ ì„¤ì • ì¤€ë¹„ (ê°œì„ ëœ ì„¤ì •)
    # --------------------------------------------------------------------------
    # ğŸ”¥ Transformer ì‚¬ìš©
    model_config = ModelConfig(
        d_model=512,             # 256 â†’ 512 (ë” í° ëª¨ë¸)
        nhead=8,                 # Multi-head attention
        num_encoder_layers=4,    # Encoder layers
        num_decoder_layers=4,    # Decoder layers
        dim_feedforward=2048,    # FFN dimension
        dropout=0.1,             # Dropout
        use_transformer=True,    # Transformer ì‚¬ìš©!
    )

    # --------------------------------------------------------------------------
    # 5) í•™ìŠµ ì„¤ì • ì¤€ë¹„ (ê°œì„ ëœ ì„¤ì •)
    # --------------------------------------------------------------------------
    train_config = TrainConfig(
        max_train_steps=None,
        lr=3e-4,                 # 2e-3 â†’ 3e-4 (ë” ì‘ì€ learning rate)
        warmup_steps=1000,       # Warmup ì¶”ê°€
        valid_every=200,
        max_gen_len=50,          # 24 â†’ 50 (ë” ê¸´ ì¶œë ¥)
        show_valid_samples=5,
        num_epochs=10,           # 50 â†’ 10 (TransformerëŠ” ë¹ ë¥´ê²Œ ìˆ˜ë ´)
        save_best_path="best_model.pt",
        label_smoothing=0.1,     # Label smoothing ì¶”ê°€
        grad_clip=1.0,
    )

    # --------------------------------------------------------------------------
    # 6) ëª¨ë¸ ì¤€ë¹„ (Transformer ë˜ëŠ” GRU)
    # --------------------------------------------------------------------------
    if model_config.use_transformer:
        # ğŸ”¥ TransformerSeq2Seq ì‚¬ìš©
        from model import TransformerSeq2Seq
        model = TransformerSeq2Seq(
            in_vocab=input_tokenizer.vocab_size,
            out_vocab=output_tokenizer.vocab_size,
            **model_config.__dict__,
        )
    else:
        # ê¸°ì¡´ GRU ëª¨ë¸
        model = TinySeq2Seq(
            in_vocab=input_tokenizer.vocab_size,
            out_vocab=output_tokenizer.vocab_size,
            **model_config.__dict__,
        )

    # --------------------------------------------------------------------------
    # 6) í•™ìŠµ ì‹œì‘
    # --------------------------------------------------------------------------

    train_loop(
        model=model,
        dataloader=train_dataloader,              # ë¯¸ë¦¬ ë§Œë“  DataLoaderë¥¼ ì§ì ‘ ì „ë‹¬í•©ë‹ˆë‹¤.
        input_tokenizer=input_tokenizer,        # (tokenize_batch ìœ í‹¸ë¦¬í‹°ë¥¼ ìœ„í•´ ì „ë‹¬)
        output_tokenizer=output_tokenizer,
        device=device,
        val_dataloader=val_dataloader,
        train_config=train_config,  # í•™ìŠµ ì„¤ì • ì „ë‹¬
        model_config=model_config,  # ëª¨ë¸ ì„¤ì • ì „ë‹¬
        tokenizer_config=tokenizer_config,  # í† í¬ë‚˜ì´ì € ì„¤ì • ì „ë‹¬
    )

    # --------------------------------------------------------------------------
    # 6) í•™ìŠµ ëë‚˜ë©´ ëª¨ë¸ ì €ì¥
    # --------------------------------------------------------------------------
    torch.save(model.state_dict(), "model.pt")
    print("Saved model.pt")


# python train.pyë¡œ ì‹¤í–‰í–ˆì„ ë•Œë§Œ main()ì„ ëŒê²Œ í•©ë‹ˆë‹¤.
if __name__ == "__main__":
    main()
